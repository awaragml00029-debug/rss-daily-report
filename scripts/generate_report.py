#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RSS Daily Report Generator
ä» Google Sheets è¯»å– RSS æ•°æ®ï¼Œç”Ÿæˆæ¯æ—¥ Markdown æŠ¥å‘Š
"""

import os
import re
import sys
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict, Any, Optional
import yaml
import gspread
from oauth2client.service_account import ServiceAccountCredentials


class RSSReportGenerator:
    """RSS æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = self._load_config(config_path)
        self.client = self._authenticate_google_sheets()
        self.sheet = None
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        # æ™ºèƒ½æŸ¥æ‰¾é…ç½®æ–‡ä»¶
        possible_paths = [
            config_path,                                    # å½“å‰ç›®å½•
            os.path.join('..', config_path),               # ä¸Šçº§ç›®å½•
            os.path.join(os.path.dirname(__file__), '..', config_path),  # è„šæœ¬çš„ä¸Šçº§ç›®å½•
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ '{config_path}'ã€‚å°è¯•è¿‡çš„è·¯å¾„: {possible_paths}"
        )
    
    def _authenticate_google_sheets(self) -> gspread.Client:
        """è®¤è¯ Google Sheets"""
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–å‡­è¯
        creds_json = os.getenv('GOOGLE_CREDENTIALS')
        if not creds_json:
            raise ValueError("æœªæ‰¾åˆ° GOOGLE_CREDENTIALS ç¯å¢ƒå˜é‡")
        
        # å°† JSON å­—ç¬¦ä¸²å†™å…¥ä¸´æ—¶æ–‡ä»¶
        import json
        creds_dict = json.loads(creds_json)
        
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(
            creds_dict, scope
        )
        return gspread.authorize(credentials)
    
    def connect_sheet(self) -> gspread.Worksheet:
        """è¿æ¥åˆ°æŒ‡å®šçš„ Google Sheet"""
        spreadsheet_id = os.getenv('SHEET_ID') or self.config['google_sheets']['spreadsheet_id']
        sheet_name = self.config['google_sheets']['sheet_name']
        
        spreadsheet = self.client.open_by_key(spreadsheet_id)
        self.sheet = spreadsheet.worksheet(sheet_name)
        return self.sheet
    
    def get_all_data(self) -> List[List[str]]:
        """è·å–æ‰€æœ‰æ•°æ®"""
        if not self.sheet:
            self.connect_sheet()
        return self.sheet.get_all_values()
    
    def parse_datetime(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        formats = [
            '%m/%d/%Y %H:%M:%S',  # 10/29/2025 0:58:55 - ä½ çš„æ ¼å¼ï¼
            '%m/%d/%Y %H:%M',     # 8/12/2025 10:30
            '%m/%d/%Y',           # 8/12/2025
            '%Y-%m-%d %H:%M:%S',  # 2025-08-12 10:30:45
            '%Y-%m-%d %H:%M',     # 2025-08-12 10:30
            '%Y-%m-%d',           # 2025-08-12
            '%Y/%m/%d',           # 2025/08/12
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•è§£ææ›´å¤æ‚çš„æ ¼å¼
        try:
            # å¤„ç†ç±»ä¼¼ "Mon, 11 Aug 2025 22:37:00 +0800" çš„æ ¼å¼
            from dateutil import parser
            return parser.parse(date_str)
        except:
            print(f"è­¦å‘Š: æ— æ³•è§£ææ—¥æœŸ '{date_str}'", file=sys.stderr)
            return None
    
    def filter_by_keywords(self, title: str) -> List[str]:
        """
        æ ¹æ®å…³é”®è¯ç­›é€‰ï¼Œè¿”å›åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        """
        if not title:
            return []
        
        keywords = self.config.get('keywords', [])
        exclude_keywords = self.config.get('exclude_keywords', [])
        
        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        for exclude_kw in exclude_keywords:
            if re.search(exclude_kw, title, re.IGNORECASE):
                return []
        
        # æ£€æŸ¥åŒ¹é…å…³é”®è¯
        matched = []
        for keyword in keywords:
            # å°†ç®€å•å…³é”®è¯è½¬æ¢ä¸ºæ­£åˆ™ï¼ˆæ”¯æŒä¸­é—´æ’å…¥å­—ç¬¦ï¼‰
            # ä¾‹å¦‚: "å•ç»†èƒ" -> "å•.*ç»†.*èƒ"
            pattern = '.*'.join(list(keyword))
            if re.search(pattern, title, re.IGNORECASE):
                matched.append(keyword)
        
        return matched
    
    def get_latest_crawl_date(self, data: List[List[str]]) -> Optional[datetime]:
        """è·å–æœ€æ–°çš„æŠ“å–æ—¥æœŸ"""
        col_idx = self.config['columns']['crawl_time'] - 1
        dates = []
        
        for row in data[1:]:  # è·³è¿‡è¡¨å¤´
            if len(row) > col_idx and row[col_idx]:
                dt = self.parse_datetime(row[col_idx])
                if dt:
                    dates.append(dt)
        
        return max(dates) if dates else None
    
    def filter_data_by_date(
        self, 
        data: List[List[str]], 
        target_date: datetime
    ) -> List[Dict[str, Any]]:
        """
        ç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼Œå¹¶è¿›è¡Œå…³é”®è¯è¿‡æ»¤
        """
        col_map = self.config['columns']
        filtered_items = []
        
        # è°ƒè¯•è®¡æ•°å™¨
        debug_counts = {
            'total_rows': len(data) - 1,
            'parsed_dates': 0,
            'matched_dates': 0,
            'matched_keywords': 0,
        }
        
        # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬ç”¨äºè°ƒè¯•
        sample_count = 0
        max_samples = 3
        
        for row in data[1:]:  # è·³è¿‡è¡¨å¤´
            if len(row) < 6:  # ç¡®ä¿æœ‰æœ€åŸºæœ¬çš„åˆ—
                continue
            
            # è·å–æŠ“å–æ—¶é—´
            crawl_time_str = row[col_map['crawl_time'] - 1]
            crawl_time = self.parse_datetime(crawl_time_str)
            
            if not crawl_time:
                continue
            
            debug_counts['parsed_dates'] += 1
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
            if sample_count < max_samples:
                print(f"ğŸ“ æ ·æœ¬ {sample_count + 1}:")
                print(f"   æŠ“å–æ—¶é—´: {crawl_time_str} -> {crawl_time}")
                print(f"   ç›®æ ‡æ—¥æœŸ: {target_date.date()}")
                print(f"   åŒ¹é…ç»“æœ: {crawl_time.date() == target_date.date()}")
                sample_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡æ—¥æœŸï¼ˆåªæ¯”è¾ƒå¹´æœˆæ—¥ï¼‰
            if crawl_time.date() != target_date.date():
                continue
            
            debug_counts['matched_dates'] += 1
            
            # è·å–æ ‡é¢˜å¹¶è¿›è¡Œå…³é”®è¯åŒ¹é…
            title = row[col_map['title'] - 1] if len(row) > col_map['title'] - 1 else ''
            matched_keywords = self.filter_by_keywords(title)
            
            # æ˜¾ç¤ºåŒ¹é…åˆ°çš„æ—¥æœŸä½†æœªåŒ¹é…å…³é”®è¯çš„å‰å‡ ä¸ª
            if not matched_keywords and debug_counts['matched_dates'] <= 3:
                print(f"ğŸ” æ—¥æœŸåŒ¹é…ä½†å…³é”®è¯æœªåŒ¹é…:")
                print(f"   æ ‡é¢˜: {title[:100]}")
                print(f"   å…³é”®è¯: {self.config.get('keywords', [])}")
            
            if not matched_keywords:  # æ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œè·³è¿‡
                continue
            
            debug_counts['matched_keywords'] += 1
            
            # æ„å»ºæ•°æ®é¡¹
            item = {
                'crawl_time': crawl_time_str,
                'attribute': row[col_map['attribute'] - 1] if len(row) > col_map['attribute'] - 1 else '',
                'source_name': row[col_map['source_name'] - 1] if len(row) > col_map['source_name'] - 1 else '',
                'category': row[col_map['category'] - 1] if len(row) > col_map['category'] - 1 else '',
                'title': title,
                'link': row[col_map['link'] - 1] if len(row) > col_map['link'] - 1 else '',
                'description': row[col_map['description'] - 1] if len(row) > col_map['description'] - 1 else '',
                'publish_time': row[col_map['publish_time'] - 1] if len(row) > col_map['publish_time'] - 1 else '',
                'author': row[col_map['author'] - 1] if len(row) > col_map['author'] - 1 else '',
                'matched_keywords': matched_keywords,
            }
            filtered_items.append(item)
        
        # è¾“å‡ºè°ƒè¯•ç»Ÿè®¡
        print(f"\nğŸ“Š ç­›é€‰ç»Ÿè®¡:")
        print(f"   æ€»è¡Œæ•°: {debug_counts['total_rows']}")
        print(f"   æˆåŠŸè§£ææ—¥æœŸ: {debug_counts['parsed_dates']}")
        print(f"   æ—¥æœŸåŒ¹é…: {debug_counts['matched_dates']}")
        print(f"   å…³é”®è¯åŒ¹é…: {debug_counts['matched_keywords']}")
        
        return filtered_items
    
    def generate_daily_report(
        self, 
        items: List[Dict[str, Any]], 
        date: datetime
    ) -> str:
        """ç”Ÿæˆæ¯æ—¥ Markdown æŠ¥å‘Š - æŒ‰æ¥æºåˆ†ç»„ï¼Œå‰10æ¡è¯¦ç»†å±•ç¤º"""
        
        # è·å–é…ç½®
        source_mapping = self.config.get('source_mapping', {})
        detail_count = self.config.get('report_format', {}).get('detail_items_per_source', 10)
        desc_max_length = self.config.get('report_format', {}).get('description_max_length', 500)
        show_more = self.config.get('report_format', {}).get('show_more_section', True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(items)
        keyword_counter = Counter()
        for item in items:
            keyword_counter.update(item['matched_keywords'])
        
        # æŒ‰æ¥æºåˆ†ç»„ï¼ˆä½¿ç”¨ category å­—æ®µï¼Œå³Dåˆ—ï¼‰
        items_by_source = {}
        for item in items:
            category = item.get('category', '').strip()
            
            # æ ¹æ®é…ç½®æ˜ å°„åˆ°æ˜¾ç¤ºåç§°
            if category in source_mapping:
                source_info = source_mapping[category]
                display_name = source_info[0]
                sort_order = source_info[1]
                icon = source_info[2]
            else:
                # æœªå®šä¹‰çš„æ¥æºä½¿ç”¨é»˜è®¤é…ç½®
                default_info = source_mapping.get('_default', ['å…¶ä»–æ¥æº', 99, 'ğŸ“'])
                display_name = default_info[0]
                sort_order = default_info[1]
                icon = default_info[2]
            
            source_key = (display_name, sort_order, icon)
            if source_key not in items_by_source:
                items_by_source[source_key] = []
            items_by_source[source_key].append(item)
        
        # æŒ‰é…ç½®çš„æ’åºé¡ºåºæ’åºæ¥æº
        sorted_sources = sorted(items_by_source.items(), key=lambda x: x[0][1])
        
        # ä¸ºæ¯ä¸ªæ¥æºçš„æ¡ç›®æ’åºï¼šå…³é”®è¯æ•°é‡ > æ—¶é—´
        for source_key, source_items in sorted_sources:
            source_items.sort(key=lambda x: (
                -len(x['matched_keywords']),  # å…³é”®è¯æ•°å¤šçš„åœ¨å‰
                -(self.parse_datetime(x['crawl_time']) or datetime.min).timestamp()  # æ—¶é—´æ–°çš„åœ¨å‰
            ))
        
        # ç”Ÿæˆ Markdown
        md_lines = []
        md_lines.append(f"# ğŸ“… Daily Report - {date.strftime('%Y-%m-%d')}")
        md_lines.append("")
        md_lines.append(f"> ä»Šæ—¥ç­›é€‰å‡º **{total_count}** æ¡å†…å®¹ï¼Œæ¥è‡ª **{len(items_by_source)}** ä¸ªæ¥æº")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # ===== åˆ†ç±»æµè§ˆ =====
        md_lines.append("## ğŸ“š åˆ†ç±»æµè§ˆ")
        md_lines.append("")
        
        # è®°å½•éœ€è¦åœ¨"æ›´å¤šå†…å®¹"åŒºåŸŸæ˜¾ç¤ºçš„æ¡ç›®
        more_items_by_source = {}
        
        for (display_name, sort_order, icon), source_items in sorted_sources:
            source_count = len(source_items)
            md_lines.append(f"### {icon} {display_name} ({source_count}æ¡)")
            md_lines.append("")
            
            # è¯¦ç»†å±•ç¤ºçš„æ¡ç›®ï¼ˆå‰Næ¡ï¼‰
            detail_items = source_items[:detail_count]
            # å‰©ä½™æ¡ç›®
            remaining_items = source_items[detail_count:]
            
            if detail_items:
                md_lines.append("#### è¯¦ç»†å†…å®¹" + (f"ï¼ˆå‰{len(detail_items)}æ¡ï¼‰" if remaining_items else f"ï¼ˆå…¨éƒ¨{len(detail_items)}æ¡ï¼‰"))
                md_lines.append("")
                
                for idx, item in enumerate(detail_items, 1):
                    keywords_str = "ã€".join(item['matched_keywords'])
                    
                    # ä¼˜å…ˆçº§æ ‡è®°
                    priority_mark = ""
                    if len(item['matched_keywords']) >= 3:
                        priority_mark = "â­ "
                    
                    # æ ‡é¢˜
                    md_lines.append(f"**{idx}.** {priority_mark}**{item['title']}**")
                    
                    # ä½œè€…ï¼ˆå¦‚æœæœ‰ï¼‰
                    if item.get('author'):
                        md_lines.append(f"- âœï¸ **ä½œè€…**ï¼š{item['author']}")
                    
                    # å…³é”®è¯
                    md_lines.append(f"- ğŸ·ï¸ **å…³é”®è¯**ï¼š{keywords_str}")
                    
                    # æè¿°
                    description = item.get('description', '').strip()
                    if description:
                        # æˆªæ–­æè¿°
                        if desc_max_length > 0 and len(description) > desc_max_length:
                            description = description[:desc_max_length] + "..."
                        md_lines.append(f"- ğŸ“ **æè¿°**ï¼š{description}")
                    
                    # é“¾æ¥
                    if item.get('link'):
                        md_lines.append(f"- ğŸ”— [æŸ¥çœ‹åŸæ–‡]({item['link']})")
                    
                    md_lines.append("")
            
            # å¦‚æœæœ‰å‰©ä½™æ¡ç›®ï¼Œæ·»åŠ æç¤º
            if remaining_items:
                anchor = display_name.replace(' ', '-').lower()
                md_lines.append(f"> ğŸ’¡ è¯¥æ¥æºè¿˜æœ‰ {len(remaining_items)} æ¡å†…å®¹ï¼Œè¯¦è§ [æ–‡æœ«](#æ›´å¤š-{anchor})")
                more_items_by_source[(display_name, icon, anchor)] = remaining_items
            
            md_lines.append("")
            md_lines.append("---")
            md_lines.append("")
        
        # ===== å…³é”®è¯ç»Ÿè®¡ =====
        md_lines.append("## ğŸ“Š å…³é”®è¯ç»Ÿè®¡")
        md_lines.append("")
        
        # ç”Ÿæˆè¡¨æ ¼
        md_lines.append("| å…³é”®è¯ | å‡ºç°æ¬¡æ•° |")
        md_lines.append("|--------|----------|")
        for keyword, count in keyword_counter.most_common(20):
            md_lines.append(f"| {keyword} | {count} |")
        
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # ===== æ›´å¤šå†…å®¹åŒºåŸŸ =====
        if show_more and more_items_by_source:
            md_lines.append("## ğŸ“ æ›´å¤šå†…å®¹")
            md_lines.append("")
            
            for (display_name, icon, anchor), remaining_items in more_items_by_source.items():
                md_lines.append(f"### <a name=\"æ›´å¤š-{anchor}\"></a>{icon} {display_name} å…¶ä»–å†…å®¹ ({len(remaining_items)}æ¡)")
                md_lines.append("")
                
                for item in remaining_items:
                    title = item['title']
                    link = item.get('link', '')
                    if link:
                        md_lines.append(f"- [{title}]({link})")
                    else:
                        md_lines.append(f"- {title}")
                
                md_lines.append("")
            
            md_lines.append("---")
            md_lines.append("")
        
        # é¡µè„š
        md_lines.append(f"*ğŸ“… æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*  ")
        md_lines.append(f"*ğŸ¤– ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(md_lines)
    
    def generate_monthly_report(
        self,
        year: int,
        month: int
    ) -> str:
        """ç”Ÿæˆæœˆåº¦æŠ¥å‘Šï¼ˆæ±‡æ€»å½“æœˆæ‰€æœ‰æ—¥æŠ¥ï¼‰"""
        
        # è·å–å½“æœˆæ‰€æœ‰æ•°æ®
        all_data = self.get_all_data()
        
        # è·å–å½“æœˆæ‰€æœ‰æ—¥æœŸ
        month_start = datetime(year, month, 1)
        if month == 12:
            month_end = datetime(year + 1, 1, 1) - timedelta(days=1)
        else:
            month_end = datetime(year, month + 1, 1) - timedelta(days=1)
        
        # æ”¶é›†å½“æœˆæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®
        all_items = []
        current_date = month_start
        while current_date <= month_end:
            daily_items = self.filter_data_by_date(all_data, current_date)
            all_items.extend(daily_items)
            current_date += timedelta(days=1)
        
        # ç»Ÿè®¡
        total_count = len(all_items)
        keyword_counter = Counter()
        for item in all_items:
            keyword_counter.update(item['matched_keywords'])
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        items_by_date = {}
        for item in all_items:
            crawl_time = self.parse_datetime(item['crawl_time'])
            if crawl_time:
                date_key = crawl_time.strftime('%Y-%m-%d')
                if date_key not in items_by_date:
                    items_by_date[date_key] = []
                items_by_date[date_key].append(item)
        
        # ç”ŸæˆæŠ¥å‘Š
        md_lines = []
        md_lines.append(f"# ğŸ“… Monthly Report - {year}å¹´{month}æœˆ")
        md_lines.append("")
        md_lines.append("> ç”Ÿç‰©ä¿¡æ¯å­¦ RSS è®¢é˜…æœˆæŠ¥")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # æœˆåº¦ç»Ÿè®¡
        md_lines.append("## ğŸ“Š æœ¬æœˆæ¦‚è§ˆ")
        md_lines.append("")
        md_lines.append(f"- ğŸ“… ç»Ÿè®¡å‘¨æœŸï¼š{month_start.strftime('%Y-%m-%d')} è‡³ {month_end.strftime('%Y-%m-%d')}")
        md_lines.append(f"- âœ… æ€»å‘½ä¸­æ¡ç›®ï¼š{total_count}")
        md_lines.append(f"- ğŸ“† æ´»è·ƒå¤©æ•°ï¼š{len(items_by_date)}")
        md_lines.append(f"- ğŸ“ˆ æ—¥å‡æ¡ç›®ï¼š{total_count // len(items_by_date) if items_by_date else 0}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # å…³é”®è¯ç»Ÿè®¡
        md_lines.append("## ğŸ·ï¸ æœ¬æœˆå…³é”®è¯ç»Ÿè®¡")
        md_lines.append("")
        for keyword, count in keyword_counter.most_common():
            md_lines.append(f"- {keyword}ï¼š{count} æ¬¡")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # æ¯æ—¥æ‘˜è¦
        md_lines.append("## ğŸ“† æ¯æ—¥æ‘˜è¦")
        md_lines.append("")
        
        for date_key in sorted(items_by_date.keys(), reverse=True):
            items = items_by_date[date_key]
            md_lines.append(f"### {date_key} ({len(items)} æ¡)")
            md_lines.append("")
            
            # åˆ—å‡ºè¯¥æ—¥çš„æ–‡ç« æ ‡é¢˜
            for item in items[:10]:  # æ¯å¤©æœ€å¤šæ˜¾ç¤º10æ¡
                keywords_str = "ã€".join(item['matched_keywords'])
                md_lines.append(f"- [{item['title']}]({item['link']}) *({keywords_str})*")
            
            if len(items) > 10:
                md_lines.append(f"- *...è¿˜æœ‰ {len(items) - 10} æ¡*")
            
            md_lines.append("")
        
        # é¡µè„š
        md_lines.append("---")
        md_lines.append("")
        md_lines.append(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*")
        
        return "\n".join(md_lines)
    
    def save_report(self, content: str, filepath: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
    
    def run_daily(self, target_date: Optional[datetime] = None):
        """è¿è¡Œæ¯æ—¥æŠ¥å‘Šç”Ÿæˆ"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š...")
        
        # è·å–æ•°æ®
        all_data = self.get_all_data()
        print(f"ğŸ“Š å…±è¯»å– {len(all_data) - 1} è¡Œæ•°æ®")
        
        # ç¡®å®šç›®æ ‡æ—¥æœŸ
        if not target_date:
            latest_date = self.get_latest_crawl_date(all_data)
            if not latest_date:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æŠ“å–æ—¥æœŸ")
                return
            target_date = latest_date
        
        print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date.strftime('%Y-%m-%d')}")
        
        # ç­›é€‰æ•°æ®
        filtered_items = self.filter_data_by_date(all_data, target_date)
        print(f"âœ… ç­›é€‰å‡º {len(filtered_items)} æ¡ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
        
        if not filtered_items:
            print("âš ï¸  æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = self.generate_daily_report(filtered_items, target_date)
        
        # ä¿å­˜æŠ¥å‘Š
        output_config = self.config['output']
        filepath = os.path.join(
            output_config['daily_path'].format(
                year=target_date.year,
                month=f"{target_date.month:02d}"
            ),
            output_config['daily_filename'].format(
                date=target_date.strftime('%Y-%m-%d')
            )
        )
        
        self.save_report(report_content, filepath)
        print("âœ¨ æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    
    def run_monthly(self, year: Optional[int] = None, month: Optional[int] = None):
        """è¿è¡Œæœˆåº¦æŠ¥å‘Šç”Ÿæˆ"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæœˆåº¦æŠ¥å‘Š...")
        
        # ç¡®å®šå¹´æœˆ
        now = datetime.now()
        if not year:
            year = now.year
        if not month:
            month = now.month
        
        print(f"ğŸ“… ç›®æ ‡æœˆä»½: {year}å¹´{month}æœˆ")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = self.generate_monthly_report(year, month)
        
        # ä¿å­˜æŠ¥å‘Š
        output_config = self.config['output']
        filepath = os.path.join(
            output_config['monthly_path'].format(year=year),
            output_config['monthly_filename'].format(
                year=year,
                month=f"{month:02d}"
            )
        )
        
        self.save_report(report_content, filepath)
        print("âœ¨ æœˆåº¦æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RSS Daily Report Generator')
    parser.add_argument(
        '--mode',
        choices=['daily', 'monthly'],
        default='daily',
        help='ç”Ÿæˆæ¨¡å¼ï¼šdailyï¼ˆæ¯æ—¥ï¼‰æˆ– monthlyï¼ˆæ¯æœˆï¼‰'
    )
    parser.add_argument(
        '--date',
        help='æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)ï¼Œä»…ç”¨äº daily æ¨¡å¼'
    )
    parser.add_argument(
        '--year',
        type=int,
        help='æŒ‡å®šå¹´ä»½ï¼Œä»…ç”¨äº monthly æ¨¡å¼'
    )
    parser.add_argument(
        '--month',
        type=int,
        help='æŒ‡å®šæœˆä»½ï¼Œä»…ç”¨äº monthly æ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = RSSReportGenerator()
    
    try:
        if args.mode == 'daily':
            # æ¯æ—¥æŠ¥å‘Š
            target_date = None
            if args.date:
                target_date = datetime.strptime(args.date, '%Y-%m-%d')
            generator.run_daily(target_date)
            
        elif args.mode == 'monthly':
            # æœˆåº¦æŠ¥å‘Š
            generator.run_monthly(args.year, args.month)
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
